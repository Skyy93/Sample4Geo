{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j0kr0017/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    " # Change to ProjectDirectory (One Directory above)\n",
    "os.chdir('/Coding/Spectrum4Geo/')\n",
    "\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from geopy.distance import geodesic\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from spectrum4geo.dataset.soundingearth import SoundingEarthDatasetEval\n",
    "from spectrum4geo.transforms import get_transforms_val_sat, get_transforms_val_spectro \n",
    "from spectrum4geo.model import TimmModel\n",
    "from spectrum4geo.trainer import predict\n",
    "\n",
    "from sklearn.metrics import DistanceMetric\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Configuration:\n",
    "    \n",
    "    # Model\n",
    "    model: str = 'convnext_base.fb_in22k_ft_in1k_384'\n",
    "    \n",
    "    # Override model image size\n",
    "    img_size: int = 384                  # for satallite images\n",
    "    patch_time_steps: int = 1024*4       # Image size for spectrograms (Width)\n",
    "    n_mels: int = 128                    # image size for spectrograms (Height)\n",
    "    sr_kHz: float = 48\n",
    "    \n",
    "    # Evaluation\n",
    "    batch_size_eval: int = 128\n",
    "    verbose: bool = True\n",
    "    gpu_ids: tuple =  (0,1,2,3)          # GPU ids for evaluating\n",
    "    normalize_features: bool = True\n",
    "    \n",
    "    # Savepath for model eval logs\n",
    "    model_path: str = './soundingearth/testing'\n",
    "\n",
    "    # Dataset\n",
    "    data_folder = 'data'        \n",
    "    split_csv = 'test_df.csv' \n",
    "\n",
    "    # Checkpoint to start from\n",
    "    checkpoint_start = 'soundingearth/training/convnext_base.fb_in22k_ft_in1k_384/145835/weights_end.pth'   \n",
    "  \n",
    "    # set num_workers to 0 if on Windows\n",
    "    num_workers: int = 0 if os.name == 'nt' else 4 \n",
    "    \n",
    "    # train on GPU if available\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "\n",
    "config = Configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: convnext_base.fb_in22k_ft_in1k_384\n",
      "Used .csv file for evaluating: test_df.csv\n",
      "{'input_size': (3, 384, 384), 'interpolation': 'bicubic', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225), 'crop_pct': 1.0, 'crop_mode': 'squash'}\n",
      "Start from: soundingearth/training/convnext_base.fb_in22k_ft_in1k_384/145835/weights_end.pth\n",
      "GPUs available: 4\n",
      "\n",
      "Spectrogram details:\n",
      "\tSample rate: 48 kHz\n",
      "\tn_mels: 128\n",
      "\tPatch width (time steps): 4096\n",
      "\n",
      "Image Size Sat: (384, 384)\n",
      "Image Size Spectro: (4096, 128)\n",
      "Mean: (0.485, 0.456, 0.406)\n",
      "Std:  (0.229, 0.224, 0.225)\n",
      "\n",
      "Satalite Images Test: 10179\n",
      "Spectrogram Images Test: 10179\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------#\n",
    "# Model                                                                       #\n",
    "#-----------------------------------------------------------------------------#\n",
    "    \n",
    "model_path = f'{config.model_path}/{config.model}/{time.strftime('%H%M%S')}'\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "\n",
    "print(f'\\nModel: {config.model}')\n",
    "\n",
    "print(f'Used .csv file for evaluating: {config.split_csv}')\n",
    "\n",
    "model = TimmModel(config.model,\n",
    "                    pretrained=True,\n",
    "                    img_size=config.img_size)\n",
    "                        \n",
    "data_config = model.get_config()\n",
    "print(data_config)\n",
    "mean = data_config['mean']\n",
    "std = data_config['std']\n",
    "img_size = config.img_size\n",
    "\n",
    "img_size_sat = (img_size, img_size)\n",
    "img_size_spectro = (config.patch_time_steps, config.n_mels)\n",
    "    \n",
    "# load pretrained Checkpoint    \n",
    "if config.checkpoint_start is not None:  \n",
    "    print('Start from:', config.checkpoint_start)\n",
    "    model_state_dict = torch.load(config.checkpoint_start)  \n",
    "    model.load_state_dict(model_state_dict, strict=False)     \n",
    "\n",
    "# Data parallel\n",
    "print('GPUs available:', torch.cuda.device_count())  \n",
    "if torch.cuda.device_count() > 1 and len(config.gpu_ids) > 1:\n",
    "    model = torch.nn.DataParallel(model, device_ids=config.gpu_ids)\n",
    "        \n",
    "# Model to device   \n",
    "model = model.to(config.device)\n",
    "\n",
    "print(f'\\nSpectrogram details:\\n'\n",
    "        f'\\tSample rate: {config.sr_kHz} kHz\\n'\n",
    "        f'\\tn_mels: {config.n_mels}\\n'\n",
    "        f'\\tPatch width (time steps): {config.patch_time_steps}')     \n",
    "\n",
    "print('\\nImage Size Sat:', img_size_sat)\n",
    "print('Image Size Spectro:', img_size_spectro)\n",
    "print(f'Mean: {mean}')\n",
    "print(f'Std:  {std}\\n') \n",
    "\n",
    "#-----------------------------------------------------------------------------#\n",
    "# DataLoader                                                                  #\n",
    "#-----------------------------------------------------------------------------#\n",
    "    \n",
    "\n",
    "# Eval\n",
    "sat_transforms_val = get_transforms_val_sat(img_size_sat,\n",
    "                                            mean=mean,\n",
    "                                            std=std,\n",
    "                                            )\n",
    "\n",
    "spectro_transforms_val = get_transforms_val_spectro(mean=mean,       \n",
    "                                                    std=std\n",
    "                                                    )        \n",
    "\n",
    "# Satalite Satellite Images\n",
    "sat_dataset_test = SoundingEarthDatasetEval(data_folder=config.data_folder ,\n",
    "                                    split_csv=config.split_csv, \n",
    "                                    query_type = 'sat',\n",
    "                                    transforms=sat_transforms_val,\n",
    "                                    patch_time_steps=config.patch_time_steps,\n",
    "                                    sr_kHz=config.sr_kHz,\n",
    "                                    n_mels=config.n_mels,\n",
    "                                    )\n",
    "\n",
    "sat_dataloader_test = DataLoader(sat_dataset_test,\n",
    "                                    batch_size=config.batch_size_eval,\n",
    "                                    num_workers=config.num_workers,\n",
    "                                    shuffle=False,\n",
    "                                    pin_memory=True)\n",
    "\n",
    "# Spectrogram Ground Images Test\n",
    "spectro_dataset_test = SoundingEarthDatasetEval(data_folder=config.data_folder ,\n",
    "                                    split_csv=config.split_csv, \n",
    "                                    query_type='spectro',\n",
    "                                    transforms=spectro_transforms_val,\n",
    "                                    patch_time_steps=config.patch_time_steps,\n",
    "                                    sr_kHz=config.sr_kHz,\n",
    "                                    n_mels=config.n_mels,\n",
    "                                    )\n",
    "\n",
    "spectro_dataloader_test = DataLoader(spectro_dataset_test,\n",
    "                                    batch_size=config.batch_size_eval,\n",
    "                                    num_workers=config.num_workers,\n",
    "                                    shuffle=False,\n",
    "                                    pin_memory=True)\n",
    "\n",
    "print('Satalite Images Test:', len(sat_dataset_test))\n",
    "print('Spectrogram Images Test:', len(spectro_dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:57<00:00,  1.38it/s]\n",
      "100%|██████████| 80/80 [01:18<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "reference_features, reference_labels, reference_chords = predict(config, model, sat_dataloader_test) \n",
    "query_features, query_labels, query_chords = predict(config, model, spectro_dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_label_ids_until_hit(query_features, reference_features, labels, step_size=1000):\n",
    "    '''returns an dict with query item label IDs as keys. Each key maps to a list of label IDs, \n",
    "       sorted by descending probability, up until (but not including) the hit (query item label ID).'''\n",
    "    Q = len(query_features)    \n",
    "    steps = Q // step_size + 1\n",
    "    labels_np = labels.cpu().numpy()\n",
    "    ref2index = {idx: i for i, idx in enumerate(labels_np)}\n",
    "    similarity = []\n",
    "    \n",
    "    for i in range(steps):\n",
    "        start = step_size * i\n",
    "        end = start + step_size\n",
    "        sim_tmp = query_features[start:end] @ reference_features.T    \n",
    "        similarity.append(sim_tmp.cpu())\n",
    "     \n",
    "    # matrix Q x R\n",
    "    similarity = torch.cat(similarity, dim=0)\n",
    "    label_ids_until_hit = {}\n",
    "    bar = tqdm(range(Q), desc='Generate lists of label_ids until Hit')\n",
    "\n",
    "    for i in bar:\n",
    "        # similiarity value of gt reference\n",
    "        gt_sim = similarity[i, ref2index[labels_np[i]]]\n",
    "        # number of references with higher similiarity as gt\n",
    "        higher_sim = (similarity[i, :] > gt_sim).numpy()\n",
    "        # creating list of label_ids until hit\n",
    "        hit_indices = np.where(higher_sim)[0]\n",
    "        # sorting in descending order\n",
    "        sorted_hit_indices = hit_indices[np.argsort(-similarity[i, hit_indices].numpy())]\n",
    "        # label_ids_until_hit[label_id] = [], [label_id1], [label_id1, label_id2, ...]\n",
    "        label_ids_until_hit[labels_np[i]] = labels_np[sorted_hit_indices].tolist()\n",
    "\n",
    "    return label_ids_until_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate lists of label_ids until Hit: 100%|██████████| 10179/10179 [00:01<00:00, 5096.24it/s]\n"
     ]
    }
   ],
   "source": [
    "labels = reference_labels\n",
    "\n",
    "# copy meta_df, reduce meta_df and and set index to 'short_key' \n",
    "meta_df = copy.deepcopy(spectro_dataset_test.meta)\n",
    "columns_to_drop = meta_df.columns.difference(['short_key', 'longitude', 'latitude', 'continent'])\n",
    "meta_df = meta_df.drop(columns=columns_to_drop)\n",
    "meta_df.set_index('short_key', inplace=True)\n",
    "\n",
    "label_ids_until_hit = calculate_label_ids_until_hit(query_features, reference_features, labels, step_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(label_ids_until_hit, metadata_df, recall_ranks, topk_recall=True, verbose=False):\n",
    "    '''returns an tuple containing an dict of recall results with the ranks as keys, the median rank, the mean error distance\n",
    "       and top_str, which is the string used as key for the Recall@~1%.'''\n",
    "    count_until_hit = [len(value) for value in label_ids_until_hit.values()]\n",
    "    id_count = len(label_ids_until_hit)\n",
    "    topk = id_count//100\n",
    "    topk_str = f'{topk}/{topk/id_count*100:0.2f}%'\n",
    "\n",
    "    #### Set up headers for display\n",
    "    if not verbose:\n",
    "        header_format = ' | '.join(['{:<13}' for _ in recall_ranks]) \n",
    "        headers = [f'Recall@{rank}' for rank in recall_ranks]\n",
    "        if topk_recall:\n",
    "            header_format += ' | {:<16}'\n",
    "            headers += [f'Recall@{topk_str}']\n",
    "        header_format += ' | {:<13}' + ' | {:<24}'\n",
    "        headers += ['Median Rank'] + ['Mean Error Distance [km]']\n",
    "        header_formated = (header_format).format(*headers)\n",
    "\n",
    "        print('Calculate Recalls, Median Rank and Mean Error Distance!')\n",
    "        print(header_formated)\n",
    "        print('-' * len(header_formated))\n",
    "\n",
    "    #### Calculating Recalls\n",
    "    recall_results = {rank: np.mean([int(count < rank) for count in count_until_hit]) * 100 for rank in recall_ranks}\n",
    "    if topk_recall:\n",
    "        recall_results[topk_str] = np.mean([int(count < topk) for count in count_until_hit]) * 100\n",
    "\n",
    "    #### Calculating Median Rank\n",
    "    median_rank = np.median(count_until_hit)\n",
    "\n",
    "    #### Calculating Mean Error Distance\n",
    "    coordinates = metadata_df.loc[:, ['latitude', 'longitude']]\n",
    "    error_distances = []\n",
    "    dist = DistanceMetric.get_metric('haversine')\n",
    "    \n",
    "    for true_label_id, wrong_label_ids in label_ids_until_hit.items():\n",
    "        true_coords = coordinates.loc[true_label_id].to_numpy()\n",
    "        if len(wrong_label_ids) > 0:\n",
    "            wrong_coords = coordinates.loc[wrong_label_ids].to_numpy()\n",
    "            # Calculate Haversine distances\n",
    "            distances = dist.pairwise(np.radians([true_coords]), np.radians(wrong_coords)).flatten()\n",
    "            error_distances.append(np.mean(distances))\n",
    "        else:\n",
    "            error_distances.append(0)\n",
    "    \n",
    "    mean_distance_error = np.mean(error_distances) * 6371  # Convert to kilometer\n",
    "\n",
    "    #### Output the calculated metrics\n",
    "    if not verbose:\n",
    "        result_format = ' | '.join(['{:<13.4f}' for _ in recall_ranks]) \n",
    "        result_values = [recall_results[rank] for rank in recall_ranks] \n",
    "        if topk_recall:\n",
    "            result_format += ' | {:<16.4f}'\n",
    "            result_values += [recall_results[topk_str]]\n",
    "        result_format += ' | {:<13.0f}' + ' | {:<24.4f}'\n",
    "        result_values += [median_rank, mean_distance_error]\n",
    "        print(result_format.format(*result_values))\n",
    "        print()\n",
    "    \n",
    "    return recall_results, median_rank, mean_distance_error, topk_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate Recalls, Median Rank and Mean Error Distance!\n",
      "Recall@1      | Recall@5      | Recall@10     | Recall@50     | Recall@100    | Recall@101/0.99% | Median Rank   | Mean Error Distance [km]\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "19.8939       | 33.5200       | 40.4755       | 56.8327       | 64.1615       | 64.2794          | 25            | 2067.6814               \n",
      "\n",
      "Calculate Recalls, Median Rank and Mean Error Distance!\n",
      "Recall@1      | Recall@5      | Recall@10     | Median Rank   | Mean Error Distance [km]\n",
      "----------------------------------------------------------------------------------------\n",
      "19.8939       | 33.5200       | 40.4755       | 25            | 2067.6814               \n",
      "\n"
     ]
    }
   ],
   "source": [
    "recall_results, median_rank, mean_distance_error, topk = calculate_scores(label_ids_until_hit, meta_df, recall_ranks=[1,5,10,50,100], topk_recall=True)\n",
    "recall_results, median_rank, mean_distance_error, topk = calculate_scores(label_ids_until_hit, meta_df, recall_ranks=[1,5,10], topk_recall=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sollte sein:\n",
    "\n",
    "Recall@1: 19.8939 - Recall@5: 33.5200 - Recall@10: 40.4755 - Recall@50: 56.8327 - Recall@100: 64.1615 - Recall@101/Recall@0.99: 64.2794\n",
    "Median Rank: 25.0\n",
    "Mean Distance: 2067.681 km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores_continentwise(label_ids_until_hit, metadata_df, recall_ranks=[1,5,10,50,100], topk_recall=True):\n",
    "    '''returns an dict with continent as keys and tuples as values. \n",
    "       These tuples contain an dict of recall results with the ranks as keys, the median rank, the mean error distance\n",
    "       and top_str, which is the string used as key for the Recall@~1%.'''\n",
    "    continents = sorted(set(metadata_df['continent']))\n",
    "    header_format = '{:<13} | ' + '{:<13} | ' + ' | '.join(['{:<13}' for _ in recall_ranks]) \n",
    "    headers = ['Continent', 'used Samples'] + [f'Recall@{rank}' for rank in recall_ranks]\n",
    "    if topk_recall:\n",
    "        header_format += ' | {:<13}' + ' | {:<8}' \n",
    "        headers += [f'Recall@~1% ->'] + [f'~1%']\n",
    "    header_format += ' | {:<13}' + ' | {:<24}'\n",
    "    headers += ['Median Rank'] + ['Mean Error Distance [km]']\n",
    "    header_formated = header_format.format(*headers)\n",
    "\n",
    "    print('Calculate Recalls, Median Rank and Mean Error Distance within Continents: ' + ', '.join(continents) + '!')\n",
    "    print(header_formated)\n",
    "    print('-' * len(header_formated))\n",
    "\n",
    "    continent_scores = {}\n",
    "    for continent in continents:\n",
    "        allowed_label_ids = set(metadata_df[metadata_df['continent'] == continent].index)\n",
    "        continent_label_ids_until_hit = {\n",
    "            key: value for key, value in label_ids_until_hit.items() if key in allowed_label_ids\n",
    "        }\n",
    "\n",
    "        for key in continent_label_ids_until_hit:\n",
    "            continent_label_ids_until_hit[key] = [\n",
    "                label_id for label_id in continent_label_ids_until_hit[key] if label_id in allowed_label_ids\n",
    "            ]\n",
    "        \n",
    "        # tuple containing an dict of recall results with the ranks as keys, the median rank, the mean error distanceand top_str\n",
    "        continent_scores[continent] = calculate_scores(continent_label_ids_until_hit, metadata_df, recall_ranks, topk_recall, verbose=True)\n",
    "        recall_results, median_rank, mean_distance_error, topk_str = continent_scores[continent]\n",
    "\n",
    "        result_format = '{:<13} | ' + '{:<13.0f} | ' + ' | '.join(['{:<13.4f}' for _ in recall_ranks]) \n",
    "        result_values = [continent, len(allowed_label_ids)] + [recall_results[rank] for rank in recall_ranks] \n",
    "        if topk_recall:\n",
    "            result_format += ' | {:<13.4f}' + ' | {:<8}' \n",
    "            result_values += [recall_results[topk_str], topk_str]\n",
    "        result_format += ' | {:<13.0f}' + ' | {:<24.4f}'\n",
    "        result_values += [median_rank, mean_distance_error]\n",
    "\n",
    "        print(result_format.format(*result_values))\n",
    "\n",
    "    print()\n",
    "\n",
    "    return continent_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate Recalls, Median Rank and Mean Error Distance for Continents: Africa, Asia, Australia, Europe, North America, Oceania, South America!\n",
      "Continent     | used Samples  | Recall@1      | Recall@5      | Recall@10     | Recall@50     | Recall@100    | Recall@~1% -> | ~1%      | Median Rank   | Mean Error Distance [km]\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Africa        | 195           | 26.6667       | 51.2821       | 61.0256       | 84.1026       | 91.7949       | 26.6667       | 1/0.51%  | 4             | 1740.9821               \n",
      "Asia          | 3104          | 17.2680       | 36.2758       | 46.9072       | 72.2938       | 80.8956       | 65.7539       | 31/1.00% | 12            | 378.2622                \n",
      "Australia     | 157           | 30.5732       | 47.1338       | 62.4204       | 85.3503       | 94.9045       | 30.5732       | 1/0.64%  | 5             | 323.9896                \n",
      "Europe        | 5634          | 21.3348       | 33.1558       | 38.8179       | 52.3252       | 59.1587       | 53.4966       | 56/0.99% | 39            | 557.5461                \n",
      "North America | 880           | 24.3182       | 37.7273       | 45.2273       | 65.4545       | 75.0000       | 43.6364       | 8/0.91%  | 15            | 1031.0170               \n",
      "Oceania       | 45            | 31.1111       | 60.0000       | 82.2222       | 100.0000      | 100.0000      | 0.0000        | 0/0.00%  | 2             | 701.8224                \n",
      "South America | 164           | 34.1463       | 51.2195       | 62.8049       | 81.7073       | 91.4634       | 34.1463       | 1/0.61%  | 4             | 1409.4532               \n",
      "\n",
      "Calculate Recalls, Median Rank and Mean Error Distance for Continents: Africa, Asia, Australia, Europe, North America, Oceania, South America!\n",
      "Continent     | used Samples  | Recall@1      | Recall@5      | Recall@10     | Recall@50     | Recall@100    | Median Rank   | Mean Error Distance [km]\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Africa        | 195           | 26.6667       | 51.2821       | 61.0256       | 84.1026       | 91.7949       | 4             | 1740.9821               \n",
      "Asia          | 3104          | 17.2680       | 36.2758       | 46.9072       | 72.2938       | 80.8956       | 12            | 378.2622                \n",
      "Australia     | 157           | 30.5732       | 47.1338       | 62.4204       | 85.3503       | 94.9045       | 5             | 323.9896                \n",
      "Europe        | 5634          | 21.3348       | 33.1558       | 38.8179       | 52.3252       | 59.1587       | 39            | 557.5461                \n",
      "North America | 880           | 24.3182       | 37.7273       | 45.2273       | 65.4545       | 75.0000       | 15            | 1031.0170               \n",
      "Oceania       | 45            | 31.1111       | 60.0000       | 82.2222       | 100.0000      | 100.0000      | 2             | 701.8224                \n",
      "South America | 164           | 34.1463       | 51.2195       | 62.8049       | 81.7073       | 91.4634       | 4             | 1409.4532               \n",
      "\n"
     ]
    }
   ],
   "source": [
    "continent_scores = calculate_scores_continentwise(label_ids_until_hit, meta_df, recall_ranks=[1,5,10,50,100], topk_recall=True)\n",
    "\n",
    "continent_scores = calculate_scores_continentwise(label_ids_until_hit, meta_df, recall_ranks=[1,5,10,50,100], topk_recall=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_region_wise_recalls(label_ids_until_hit, metadata_df, calc_ranks=[1,5,10], print_ranks=[1,5,10]):\n",
    "    '''returns an dict where each key is a rank (from calc_ranks) and the value is another dict.\n",
    "       The nested dict has continents as keys and region-wise recall scores as values.'''\n",
    "    continents = sorted(set(metadata_df['continent']))\n",
    "\n",
    "    print('Calculate RegionWiseRecalls!')\n",
    "    if print_ranks:\n",
    "        header_format = '{:<15} | {:<14} | ' + ' | '.join(['{:<19}' for _ in print_ranks])\n",
    "        headers = ['Continent', 'valid Samples'] + [f'RegionWiseRecall@{rank}' for rank in print_ranks]\n",
    "        header_formated = (header_format).format(*headers)\n",
    "        print(header_formated)\n",
    "        print('-' * (len(header_formated)))\n",
    "\n",
    "    region_wise_recalls = {}\n",
    "    for continent in continents:\n",
    "        allowed_label_ids = set(metadata_df[metadata_df['continent'] == continent].index)\n",
    "\n",
    "        label_ids_until_continent_hit = {}\n",
    "        for key, wrong_label_ids in label_ids_until_hit.items():\n",
    "            if key in allowed_label_ids:\n",
    "                continental_wrong_label_ids = next((wrong_label_ids[:i] for i, id in enumerate(wrong_label_ids) if id in allowed_label_ids), wrong_label_ids)\n",
    "                label_ids_until_continent_hit[key] = continental_wrong_label_ids\n",
    "\n",
    "        count_until_continent_hit = [len(value) for value in label_ids_until_continent_hit.values()]\n",
    "\n",
    "        recall_results = {}\n",
    "        for rank in calc_ranks:\n",
    "            region_wise_recall = np.mean([int(count < rank) for count in count_until_continent_hit])*100\n",
    "            recall_results[rank] = region_wise_recall\n",
    "\n",
    "        region_wise_recalls[continent] = recall_results\n",
    "\n",
    "        if print_ranks:\n",
    "            result_format = '{:<15} | {:<14} | ' + ' | '.join(['{:<19.4f}' for _ in print_ranks])\n",
    "            result_values = [continent, len(allowed_label_ids)] + [region_wise_recalls[continent].get(rank, 0.0) for rank in print_ranks]\n",
    "            print(result_format.format(*result_values))\n",
    "    print()\n",
    "    \n",
    "    return region_wise_recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate RegionWiseRecalls!\n",
      "Continent       | valid Samples  | RegionWiseRecall@1  | RegionWiseRecall@5  | RegionWiseRecall@10 | RegionWiseRecall@25\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Africa          | 195            | 46.6667             | 62.5641             | 71.7949             | 82.5641            \n",
      "Asia            | 3104           | 91.9137             | 94.5554             | 95.8441             | 97.4871            \n",
      "Australia       | 157            | 45.2229             | 63.6943             | 69.4268             | 76.4331            \n",
      "Europe          | 5634           | 89.9894             | 98.1186             | 98.9883             | 99.6273            \n",
      "North America   | 880            | 56.7045             | 76.7045             | 84.3182             | 93.4091            \n",
      "Oceania         | 45             | 42.2222             | 48.8889             | 57.7778             | 64.4444            \n",
      "South America   | 164            | 40.2439             | 59.1463             | 67.6829             | 75.0000            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "region_wise_recalls = calculate_region_wise_recalls(label_ids_until_hit, meta_df, calc_ranks=[1,5,10,25], print_ranks=[1,5,10,25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_blanced_continental_recalls(region_wise_recalls):\n",
    "    '''returns an dict with continents as keys and balanced recall scores as values.'''\n",
    "    print('Calculate BalancedContinentalRecalls!')\n",
    "\n",
    "    # inverts region_wise_recalls dictionary from {continent: {rank: value}} to {rank: {continent: value}}\n",
    "    ranks = sorted(set(rank for subdict in region_wise_recalls.values() for rank in subdict.keys()))\n",
    "    region_wise_recalls = {rank: {continent: region_wise_recalls[continent].get(rank, None) for continent in region_wise_recalls} for rank in ranks}\n",
    "\n",
    "    header_format = ' | '.join(['{:<29}' for _ in ranks])\n",
    "    headers = [f'BalancedContinentalRecall@{rank}' for rank in ranks]\n",
    "    header_formated = (header_format).format(*headers)\n",
    "    print(header_formated)\n",
    "    print('-' * (len(header_formated)))\n",
    "\n",
    "    balanced_continental_recalls = {}\n",
    "    for rank in ranks:\n",
    "        recall_values = [value for value in region_wise_recalls[rank].values()]\n",
    "        balanced_continental_recalls[rank] = np.mean(recall_values)\n",
    "\n",
    "    result_format = ' | '.join(['{:<29.4f}' for _ in ranks])\n",
    "    result_values = [balanced_continental_recalls[rank] for rank in ranks]\n",
    "    print(result_format.format(*result_values))\n",
    "    print()\n",
    "\n",
    "    return balanced_continental_recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate BalancedContinentalRecalls!\n",
      "BalancedContinentalRecall@1   | BalancedContinentalRecall@5   | BalancedContinentalRecall@10  | BalancedContinentalRecall@25 \n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "58.9948                       | 71.9532                       | 77.9761                       | 84.1379                      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "blanced_continental_recalls = calculate_blanced_continental_recalls(region_wise_recalls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
