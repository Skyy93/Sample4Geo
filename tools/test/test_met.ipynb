{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_label_ids_until_hit(query_features, reference_features, labels, step_size=1000):\n",
    "    Q = len(query_features)    \n",
    "    steps = Q // step_size + 1\n",
    "    \n",
    "    labels_np = labels.cpu().numpy()\n",
    "    ref2index = {idx: i for i, idx in enumerate(labels_np)}\n",
    "    \n",
    "    similarity = []\n",
    "    \n",
    "    for i in range(steps):\n",
    "        start = step_size * i\n",
    "        end = start + step_size\n",
    "        sim_tmp = query_features[start:end] @ reference_features.T    \n",
    "        similarity.append(sim_tmp.cpu())\n",
    "     \n",
    "    # matrix Q x R\n",
    "    similarity = torch.cat(similarity, dim=0)\n",
    "    label_ids_until_hit = dict()\n",
    "    bar = tqdm(range(Q), desc=\"Generate lists of label_ids until Hit\")\n",
    "\n",
    "    for i in bar:\n",
    "        # similiarity value of gt reference\n",
    "        gt_sim = similarity[i, ref2index[labels_np[i]]]\n",
    "\n",
    "        # number of references with higher similiarity as gt\n",
    "        higher_sim = (similarity[i, :] > gt_sim).numpy()\n",
    "\n",
    "        # creating list of label_ids until hit\n",
    "        hit_indices = np.where(higher_sim)[0]\n",
    "        \n",
    "        # sorting in descending order\n",
    "        sorted_hit_indices = hit_indices[np.argsort(-similarity[i, hit_indices].numpy())]\n",
    "\n",
    "        # label_ids_until_hit[label_id] = [], [label_id1], [label_id1, label_id2, ...]\n",
    "        label_ids_until_hit[labels_np[i]] = labels_np[sorted_hit_indices].tolist()\n",
    "\n",
    "    return label_ids_until_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(label_ids_until_hit, metadata_df, recall_ranks, topk_recall=True ):\n",
    "    #### Calculating Recalls\n",
    "    count_until_hit = [len(value) for value in label_ids_until_hit.values()]\n",
    "\n",
    "    recall_results = list()\n",
    "    for rank in recall_ranks:\n",
    "        recall = np.mean([int(count < rank) for count in count_until_hit])*100\n",
    "        info = f\"Recall@{rank}\"\n",
    "        recall_results.append((recall, info))\n",
    "\n",
    "    if topk_recall:\n",
    "        id_count = len(label_ids_until_hit)\n",
    "        topk = id_count//100\n",
    "        recall = np.mean([int(count < topk)  for count in count_until_hit])*100\n",
    "        info = f\"Recall@{topk}/Recall@{topk/id_count*100:0.2f}\"\n",
    "        recall_results.append((recall, info))\n",
    "\n",
    "    print(' - '.join([f\"{info}: {recall:.4f}\" for recall, info in recall_results]))\n",
    "\n",
    "    #### Calculating Median Rank\n",
    "    median_rank = np.median(count_until_hit)\n",
    "    print('Median Rank: {:.4f}'.format(median_rank))\n",
    "\n",
    "#    ##### Calculating Mean Error Distance\n",
    "#    coordinates = metadata_df.loc[:, [\"latitude\", \"longitude\"]]\n",
    "#    error_distances = []\n",
    "#    \n",
    "#    # Initialize Haversine Distance Metric\n",
    "#    dist = DistanceMetric.get_metric(\"haversine\")\n",
    "#    \n",
    "#    for true_label_id, wrong_label_ids in label_ids_until_hit.items():\n",
    "#        true_coords = coordinates.loc[true_label_id].to_numpy()\n",
    "#        if len(wrong_label_ids) > 0:\n",
    "#            wrong_coords = coordinates.loc[wrong_label_ids].to_numpy()\n",
    "#            # Calculate Haversine distances\n",
    "#            distances = dist.pairwise(np.radians([true_coords]), np.radians(wrong_coords)).flatten() * 6371  \n",
    "#            error_distances.append(np.mean(distances))\n",
    "#        else:\n",
    "#            error_distances.append(0)\n",
    "#    \n",
    "#    mean_distance_error = np.mean(error_distances)\n",
    "#    print(f\"Mean Distance: {mean_distance_error:.3f} km\")\n",
    "    \n",
    "    return recall_results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate lists of label_ids until Hit: 100%|██████████| 100/100 [00:00<00:00, 13069.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1: 2.0000 - Recall@5: 5.0000 - Recall@10: 12.0000 - Recall@50: 50.0000 - Recall@100: 100.0000 - Recall@1/Recall@1.00: 2.0000\n",
      "Median Rank: 48.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query_features = torch.randn(100,512)\n",
    "reference_features = torch.randn(100,512)\n",
    "labels = torch.tensor(range(1,101))\n",
    "\n",
    "label_ids_until_hit = calculate_label_ids_until_hit(query_features, reference_features, labels, step_size=1000)\n",
    "recall_1 = calculate_scores(label_ids_until_hit, None, recall_ranks=[1,5,10,50,100], topk_recall=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
